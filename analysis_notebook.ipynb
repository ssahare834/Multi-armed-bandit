{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Algorithms: Mathematical Analysis\n",
    "\n",
    "This notebook provides detailed mathematical derivations and ablation studies for the three bandit algorithms implemented in this project.\n",
    "\n",
    "**Author**: Your Name  \n",
    "**Date**: January 2026  \n",
    "**Project**: News Recommendation System with MAB Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from bandit_algorithms import EpsilonGreedy, UCB, ThompsonSampling\n",
    "from simulation import NewsEnvironment, run_simulation, compare_algorithms\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Formulation\n",
    "\n",
    "### 1.1 The Multi-Armed Bandit Problem\n",
    "\n",
    "We have $K$ arms (articles), each with unknown reward distribution. At each time step $t$:\n",
    "\n",
    "1. Select arm $a_t \\in \\{1, 2, ..., K\\}$\n",
    "2. Observe reward $r_t \\sim P(r|a_t)$\n",
    "3. Update beliefs about arm rewards\n",
    "\n",
    "**Goal**: Maximize cumulative reward $\\sum_{t=1}^{T} r_t$\n",
    "\n",
    "### 1.2 Regret Definition\n",
    "\n",
    "Let $\\mu^* = \\max_a \\mu_a$ be the optimal mean reward.\n",
    "\n",
    "The **regret** at time $T$ is:\n",
    "\n",
    "$$R_T = T\\mu^* - \\sum_{t=1}^{T} r_t$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$R_T = \\sum_{a=1}^{K} \\Delta_a \\mathbb{E}[N_a(T)]$$\n",
    "\n",
    "where:\n",
    "- $\\Delta_a = \\mu^* - \\mu_a$ (suboptimality gap)\n",
    "- $N_a(T)$ = number of times arm $a$ was pulled in $T$ rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Epsilon-Greedy Algorithm\n",
    "\n",
    "### 2.1 Algorithm Description\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "$$a_t = \\begin{cases}\n",
    "\\text{random arm} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a \\hat{\\mu}_a(t) & \\text{with probability } 1-\\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\hat{\\mu}_a(t) = \\frac{1}{N_a(t)} \\sum_{s=1}^{t} r_s \\mathbb{1}(a_s = a)$\n",
    "\n",
    "### 2.2 Regret Analysis\n",
    "\n",
    "**Theorem**: For constant $\\epsilon$, Epsilon-Greedy has **linear regret**:\n",
    "\n",
    "$$\\mathbb{E}[R_T] = \\Omega(T)$$\n",
    "\n",
    "**Proof sketch**:\n",
    "- With probability $\\epsilon$, we explore uniformly\n",
    "- Expected pulls of suboptimal arm $a$: $\\mathbb{E}[N_a(T)] \\geq \\frac{\\epsilon T}{K}$\n",
    "- Regret contribution: $\\Delta_a \\cdot \\frac{\\epsilon T}{K}$\n",
    "- Total regret: $R_T \\geq \\frac{\\epsilon T}{K} \\sum_{a: \\Delta_a > 0} \\Delta_a = \\Omega(T)$\n",
    "\n",
    "### 2.3 Decaying Epsilon\n",
    "\n",
    "To achieve sublinear regret, use $\\epsilon_t = \\frac{c}{t^\\alpha}$ with $0 < \\alpha < 1$:\n",
    "\n",
    "$$\\mathbb{E}[R_T] = O(T^{1-\\alpha})$$\n",
    "\n",
    "Common choice: $\\epsilon_t = \\min\\{1, \\frac{cK}{d^2 t}\\}$ where $c > 0$, $d = \\min_{a: \\Delta_a > 0} \\Delta_a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Study 1: Effect of Epsilon on Performance\n",
    "\n",
    "def epsilon_ablation_study(epsilon_values, n_rounds=1000, n_simulations=50):\n",
    "    \"\"\"\n",
    "    Study the effect of different epsilon values on performance\n",
    "    \"\"\"\n",
    "    env = NewsEnvironment(n_articles=10, seed=42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for epsilon in epsilon_values:\n",
    "        print(f\"Testing epsilon={epsilon}...\")\n",
    "        \n",
    "        total_rewards = []\n",
    "        final_regrets = []\n",
    "        exploration_ratios = []\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            algo = EpsilonGreedy(n_arms=10, epsilon=epsilon)\n",
    "            sim_results = run_simulation(algo, env, n_rounds)\n",
    "            \n",
    "            total_rewards.append(sum(sim_results['rewards']))\n",
    "            final_regrets.append(sim_results['regret'][-1])\n",
    "            exploration_ratios.append(algo.get_exploration_ratio())\n",
    "        \n",
    "        results.append({\n",
    "            'epsilon': epsilon,\n",
    "            'avg_reward': np.mean(total_rewards),\n",
    "            'std_reward': np.std(total_rewards),\n",
    "            'avg_regret': np.mean(final_regrets),\n",
    "            'std_regret': np.std(final_regrets),\n",
    "            'avg_exploration_ratio': np.mean(exploration_ratios)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run ablation\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5]\n",
    "epsilon_results = epsilon_ablation_study(epsilon_values)\n",
    "\n",
    "print(epsilon_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epsilon ablation\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average reward vs epsilon\n",
    "axes[0].errorbar(epsilon_results['epsilon'], epsilon_results['avg_reward'],\n",
    "                yerr=epsilon_results['std_reward'], marker='o', capsize=5)\n",
    "axes[0].set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "axes[0].set_ylabel('Average Total Reward', fontsize=12)\n",
    "axes[0].set_title('Effect of Epsilon on Total Reward', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average regret vs epsilon\n",
    "axes[1].errorbar(epsilon_results['epsilon'], epsilon_results['avg_regret'],\n",
    "                yerr=epsilon_results['std_regret'], marker='o', capsize=5, color='red')\n",
    "axes[1].set_xlabel('Epsilon (ε)', fontsize=12)\n",
    "axes[1].set_ylabel('Average Final Regret', fontsize=12)\n",
    "axes[1].set_title('Effect of Epsilon on Cumulative Regret', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal epsilon\n",
    "optimal_idx = epsilon_results['avg_regret'].idxmin()\n",
    "optimal_epsilon = epsilon_results.loc[optimal_idx, 'epsilon']\n",
    "print(f\"\\nOptimal epsilon: {optimal_epsilon}\")\n",
    "print(f\"Average regret at optimal epsilon: {epsilon_results.loc[optimal_idx, 'avg_regret']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upper Confidence Bound (UCB) Algorithm\n",
    "\n",
    "### 3.1 Algorithm Description\n",
    "\n",
    "UCB1 selects arm with highest upper confidence bound:\n",
    "\n",
    "$$a_t = \\arg\\max_a \\left[ \\hat{\\mu}_a(t) + \\sqrt{\\frac{2\\ln t}{N_a(t)}} \\right]$$\n",
    "\n",
    "The second term is the **exploration bonus** based on:\n",
    "- **Optimism in the face of uncertainty**: Assume uncertain arms might be best\n",
    "- **Confidence radius**: Decreases as we learn more about each arm\n",
    "\n",
    "### 3.2 Hoeffding's Inequality Foundation\n",
    "\n",
    "UCB is based on Hoeffding's inequality:\n",
    "\n",
    "$$P(|\\hat{\\mu}_a - \\mu_a| \\geq u) \\leq 2e^{-2N_a u^2}$$\n",
    "\n",
    "Setting $2e^{-2N_a u^2} = \\frac{1}{t^4}$ and solving for $u$:\n",
    "\n",
    "$$u = \\sqrt{\\frac{2\\ln t + c}{N_a}}$$\n",
    "\n",
    "This gives us confidence that $\\mu_a \\in [\\hat{\\mu}_a - u, \\hat{\\mu}_a + u]$ with high probability.\n",
    "\n",
    "### 3.3 Regret Bound\n",
    "\n",
    "**Theorem (Auer et al., 2002)**: UCB1 achieves **logarithmic regret**:\n",
    "\n",
    "$$\\mathbb{E}[R_T] \\leq \\sum_{a: \\Delta_a > 0} \\left( \\frac{8\\ln T}{\\Delta_a} + \\left(1 + \\frac{\\pi^2}{3}\\right) \\Delta_a \\right)$$\n",
    "\n",
    "This is asymptotically optimal!\n",
    "\n",
    "**Key insight**: Regret grows as $O(\\log T)$ rather than $O(T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation Study 2: Effect of UCB Confidence Parameter\n",
    "\n",
    "def ucb_c_ablation_study(c_values, n_rounds=1000, n_simulations=50):\n",
    "    \"\"\"\n",
    "    Study the effect of different c parameter values on UCB performance\n",
    "    \"\"\"\n",
    "    env = NewsEnvironment(n_articles=10, seed=42)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for c in c_values:\n",
    "        print(f\"Testing c={c}...\")\n",
    "        \n",
    "        total_rewards = []\n",
    "        final_regrets = []\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            algo = UCB(n_arms=10, c=c)\n",
    "            sim_results = run_simulation(algo, env, n_rounds)\n",
    "            \n",
    "            total_rewards.append(sum(sim_results['rewards']))\n",
    "            final_regrets.append(sim_results['regret'][-1])\n",
    "        \n",
    "        results.append({\n",
    "            'c': c,\n",
    "            'avg_reward': np.mean(total_rewards),\n",
    "            'std_reward': np.std(total_rewards),\n",
    "            'avg_regret': np.mean(final_regrets),\n",
    "            'std_regret': np.std(final_regrets)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run ablation\n",
    "c_values = [0.5, 1.0, 1.414, 2.0, 3.0, 5.0]\n",
    "ucb_results = ucb_c_ablation_study(c_values)\n",
    "\n",
    "print(ucb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UCB c parameter ablation\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average reward vs c\n",
    "axes[0].errorbar(ucb_results['c'], ucb_results['avg_reward'],\n",
    "                yerr=ucb_results['std_reward'], marker='s', capsize=5, color='green')\n",
    "axes[0].axvline(x=np.sqrt(2), color='red', linestyle='--', label='√2 (theoretical)')\n",
    "axes[0].set_xlabel('Confidence Parameter (c)', fontsize=12)\n",
    "axes[0].set_ylabel('Average Total Reward', fontsize=12)\n",
    "axes[0].set_title('Effect of c on UCB Total Reward', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average regret vs c\n",
    "axes[1].errorbar(ucb_results['c'], ucb_results['avg_regret'],\n",
    "                yerr=ucb_results['std_regret'], marker='s', capsize=5, color='orange')\n",
    "axes[1].axvline(x=np.sqrt(2), color='red', linestyle='--', label='√2 (theoretical)')\n",
    "axes[1].set_xlabel('Confidence Parameter (c)', fontsize=12)\n",
    "axes[1].set_ylabel('Average Final Regret', fontsize=12)\n",
    "axes[1].set_title('Effect of c on UCB Cumulative Regret', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thompson Sampling\n",
    "\n",
    "### 4.1 Bayesian Framework\n",
    "\n",
    "Thompson Sampling takes a **Bayesian approach**:\n",
    "\n",
    "1. Maintain posterior distribution over each arm's mean reward: $P(\\mu_a | \\text{data})$\n",
    "2. Sample $\\tilde{\\mu}_a \\sim P(\\mu_a | \\text{data})$ for each arm\n",
    "3. Select $a_t = \\arg\\max_a \\tilde{\\mu}_a$\n",
    "\n",
    "### 4.2 Beta-Bernoulli Conjugacy\n",
    "\n",
    "For binary rewards (click/no-click), we use **Beta-Bernoulli conjugacy**:\n",
    "\n",
    "**Prior**: $\\mu_a \\sim \\text{Beta}(\\alpha_0, \\beta_0)$\n",
    "\n",
    "**Likelihood**: $r \\sim \\text{Bernoulli}(\\mu_a)$\n",
    "\n",
    "**Posterior**: $\\mu_a | \\text{data} \\sim \\text{Beta}(\\alpha_0 + S_a, \\beta_0 + F_a)$\n",
    "\n",
    "where:\n",
    "- $S_a$ = number of successes (clicks) for arm $a$\n",
    "- $F_a$ = number of failures (no clicks) for arm $a$\n",
    "\n",
    "### 4.3 Beta Distribution Properties\n",
    "\n",
    "The Beta distribution $\\text{Beta}(\\alpha, \\beta)$ has:\n",
    "\n",
    "**PDF**: $f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}$\n",
    "\n",
    "**Mean**: $\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "**Variance**: $\\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$\n",
    "\n",
    "**Mode**: $\\text{mode}(X) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}$ (for $\\alpha, \\beta > 1$)\n",
    "\n",
    "### 4.4 Regret Bound\n",
    "\n",
    "**Theorem (Agrawal & Goyal, 2012)**: Thompson Sampling achieves:\n",
    "\n",
    "$$\\mathbb{E}[R_T] = O\\left(\\sum_{a: \\Delta_a > 0} \\frac{\\ln T}{\\Delta_a}\\right)$$\n",
    "\n",
    "This matches the lower bound up to constant factors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Thompson Sampling Beta Distributions Evolution\n",
    "\n",
    "def visualize_thompson_evolution(n_rounds=100):\n",
    "    \"\"\"\n",
    "    Visualize how Beta distributions evolve over time\n",
    "    \"\"\"\n",
    "    env = NewsEnvironment(n_articles=5, seed=42)\n",
    "    algo = ThompsonSampling(n_arms=5)\n",
    "    \n",
    "    # Snapshots at different time points\n",
    "    snapshots = [10, 30, 50, 100]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    x = np.linspace(0, 1, 200)\n",
    "    \n",
    "    for snap_idx, snap_round in enumerate(snapshots):\n",
    "        algo.reset()\n",
    "        \n",
    "        # Run simulation until snapshot\n",
    "        for t in range(snap_round):\n",
    "            arm = algo.select_arm()\n",
    "            reward = env.pull_arm(arm)\n",
    "            algo.update(arm, reward)\n",
    "        \n",
    "        # Plot Beta distributions\n",
    "        ax = axes[snap_idx]\n",
    "        \n",
    "        for arm in range(5):\n",
    "            alpha, beta = algo.alpha[arm], algo.beta[arm]\n",
    "            y = stats.beta.pdf(x, alpha, beta)\n",
    "            \n",
    "            label = f'Arm {arm} (α={alpha:.1f}, β={beta:.1f})'\n",
    "            ax.plot(x, y, label=label, linewidth=2)\n",
    "            \n",
    "            # Mark true CTR\n",
    "            true_ctr = env.true_ctrs[arm]\n",
    "            ax.axvline(true_ctr, linestyle='--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        ax.set_xlabel('CTR', fontsize=11)\n",
    "        ax.set_ylabel('Probability Density', fontsize=11)\n",
    "        ax.set_title(f'After {snap_round} rounds', fontsize=13)\n",
    "        ax.legend(fontsize=8, loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_thompson_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis\n",
    "\n",
    "### 5.1 Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison across different time horizons\n",
    "\n",
    "def comprehensive_comparison(time_horizons, n_simulations=100):\n",
    "    \"\"\"\n",
    "    Compare all algorithms across different time horizons\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for T in time_horizons:\n",
    "        print(f\"\\nTesting T={T} rounds...\")\n",
    "        \n",
    "        env = NewsEnvironment(n_articles=10, seed=42)\n",
    "        \n",
    "        algorithms = {\n",
    "            'Epsilon-Greedy': EpsilonGreedy(10, epsilon=0.1),\n",
    "            'UCB': UCB(10, c=np.sqrt(2)),\n",
    "            'Thompson Sampling': ThompsonSampling(10)\n",
    "        }\n",
    "        \n",
    "        comparison = compare_algorithms(algorithms, env, T, n_simulations)\n",
    "        comparison['Time Horizon'] = T\n",
    "        \n",
    "        results.append(comparison)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Run comprehensive comparison\n",
    "time_horizons = [100, 500, 1000, 2000, 5000]\n",
    "comparison_df = comprehensive_comparison(time_horizons, n_simulations=50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling behavior\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot regret vs time horizon\n",
    "for algo_name in ['Epsilon-Greedy', 'UCB', 'Thompson Sampling']:\n",
    "    algo_data = comparison_df[comparison_df['Algorithm'] == algo_name]\n",
    "    \n",
    "    axes[0].errorbar(\n",
    "        algo_data['Time Horizon'],\n",
    "        algo_data['Avg Final Regret'],\n",
    "        yerr=algo_data['Std Final Regret'],\n",
    "        marker='o',\n",
    "        label=algo_name,\n",
    "        capsize=4,\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Time Horizon (T)', fontsize=12)\n",
    "axes[0].set_ylabel('Average Cumulative Regret', fontsize=12)\n",
    "axes[0].set_title('Regret Growth over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot average CTR vs time horizon\n",
    "for algo_name in ['Epsilon-Greedy', 'UCB', 'Thompson Sampling']:\n",
    "    algo_data = comparison_df[comparison_df['Algorithm'] == algo_name]\n",
    "    \n",
    "    axes[1].plot(\n",
    "        algo_data['Time Horizon'],\n",
    "        algo_data['Avg CTR'],\n",
    "        marker='o',\n",
    "        label=algo_name,\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Add optimal CTR line\n",
    "env = NewsEnvironment(n_articles=10, seed=42)\n",
    "axes[1].axhline(env.optimal_ctr, color='red', linestyle='--', \n",
    "               label='Optimal CTR', linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Time Horizon (T)', fontsize=12)\n",
    "axes[1].set_ylabel('Average CTR', fontsize=12)\n",
    "axes[1].set_title('CTR Convergence', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis\n",
    "\n",
    "### 6.1 Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing\n",
    "\n",
    "def run_significance_tests(n_rounds=1000, n_simulations=100):\n",
    "    \"\"\"\n",
    "    Run paired t-tests to determine if differences are statistically significant\n",
    "    \"\"\"\n",
    "    env = NewsEnvironment(n_articles=10, seed=42)\n",
    "    \n",
    "    # Collect results\n",
    "    eg_regrets = []\n",
    "    ucb_regrets = []\n",
    "    ts_regrets = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Epsilon-Greedy\n",
    "        algo_eg = EpsilonGreedy(10, epsilon=0.1)\n",
    "        result_eg = run_simulation(algo_eg, env, n_rounds)\n",
    "        eg_regrets.append(result_eg['regret'][-1])\n",
    "        \n",
    "        # UCB\n",
    "        algo_ucb = UCB(10, c=np.sqrt(2))\n",
    "        result_ucb = run_simulation(algo_ucb, env, n_rounds)\n",
    "        ucb_regrets.append(result_ucb['regret'][-1])\n",
    "        \n",
    "        # Thompson Sampling\n",
    "        algo_ts = ThompsonSampling(10)\n",
    "        result_ts = run_simulation(algo_ts, env, n_rounds)\n",
    "        ts_regrets.append(result_ts['regret'][-1])\n",
    "    \n",
    "    # Perform t-tests\n",
    "    print(\"\\nPaired T-Tests (Cumulative Regret)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Thompson vs Epsilon-Greedy\n",
    "    t_stat, p_val = stats.ttest_rel(ts_regrets, eg_regrets)\n",
    "    print(f\"Thompson Sampling vs Epsilon-Greedy:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.6f}\")\n",
    "    print(f\"  Significant at α=0.05: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "    print()\n",
    "    \n",
    "    # Thompson vs UCB\n",
    "    t_stat, p_val = stats.ttest_rel(ts_regrets, ucb_regrets)\n",
    "    print(f\"Thompson Sampling vs UCB:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.6f}\")\n",
    "    print(f\"  Significant at α=0.05: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "    print()\n",
    "    \n",
    "    # UCB vs Epsilon-Greedy\n",
    "    t_stat, p_val = stats.ttest_rel(ucb_regrets, eg_regrets)\n",
    "    print(f\"UCB vs Epsilon-Greedy:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_val:.6f}\")\n",
    "    print(f\"  Significant at α=0.05: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "    \n",
    "    return eg_regrets, ucb_regrets, ts_regrets\n",
    "\n",
    "eg_regrets, ucb_regrets, ts_regrets = run_significance_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "data = [eg_regrets, ucb_regrets, ts_regrets]\n",
    "labels = ['Epsilon-Greedy', 'UCB', 'Thompson Sampling']\n",
    "\n",
    "bp = ax.boxplot(data, labels=labels, patch_artist=True,\n",
    "                notch=True, showmeans=True)\n",
    "\n",
    "# Color boxes\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Cumulative Regret', fontsize=12)\n",
    "ax.set_title('Distribution of Cumulative Regret across 100 Simulations', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Thompson Sampling** typically achieves the **lowest regret** in practice\n",
    "   - Naturally balances exploration and exploitation\n",
    "   - Adapts uncertainty estimates over time\n",
    "   - Often statistically significantly better than alternatives\n",
    "\n",
    "2. **UCB** provides **strong theoretical guarantees**\n",
    "   - Logarithmic regret bound\n",
    "   - Deterministic (no randomness)\n",
    "   - Performance depends on choosing appropriate $c$ parameter\n",
    "\n",
    "3. **Epsilon-Greedy** is **simple but less efficient**\n",
    "   - Linear regret with constant epsilon\n",
    "   - Can achieve sublinear regret with decaying epsilon\n",
    "   - Good baseline, easy to understand\n",
    "\n",
    "### Practical Recommendations:\n",
    "\n",
    "- **Use Thompson Sampling** for best empirical performance\n",
    "- **Use UCB** when theoretical guarantees are important\n",
    "- **Use Epsilon-Greedy** for quick prototypes or when simplicity matters\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "1. **Contextual bandits** with user/article features\n",
    "2. **Non-stationary** environments (CTRs change over time)\n",
    "3. **Batch updates** for computational efficiency\n",
    "4. **Delayed feedback** in real systems\n",
    "5. **Fairness constraints** in recommendations\n",
    "\n",
    "---\n",
    "\n",
    "**References**:\n",
    "\n",
    "1. Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem.\n",
    "2. Agrawal, S., & Goyal, N. (2012). Analysis of Thompson sampling for the multi-armed bandit problem.\n",
    "3. Chapelle, O., & Li, L. (2011). An empirical evaluation of Thompson sampling.\n",
    "4. Lattimore, T., & Szepesvári, C. (2020). Bandit Algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
